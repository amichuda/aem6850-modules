{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using OpenAI API and GPT Models\n",
    "\n",
    "- Admin\n",
    "    - Presentations\n",
    "    - Next week lecture, over Zoom\n",
    "    - Peer Reviews\n",
    "    - `libmamba`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \"AI,\" whatever that means, has been progressing for many years and now and OpenAI finally reached a milestone in development\n",
    "- ChatGPT has been becoming a very important part of life\n",
    "- It will probably be an important of analysis in the future as well\n",
    "- You can use ChatGPT to ask questions\n",
    "    - But what if you wanted to use this for data?\n",
    "-  What can GPT models do for you?\n",
    "    - Entity Recognition\n",
    "    - Sentiment Analysis\n",
    "    - Text segmentation\n",
    "    - Translating Text, maybe?\n",
    "    - Anything with text basically\n",
    "- These are all things that you can train yourself, it's just OpenAI has already done A LOT of the heavy lifting.\n",
    "- You can even train your own models using theirs as a base\n",
    "    - But beware that this requires a lot of data!\n",
    "- Also note: the way you write prompts if very important\n",
    "    - There's an emerging art for how to write a prompt for GPT models so that they give you what you need\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entity Recognition\n",
    "\n",
    "- Finds information in your text\n",
    "- Useful for parsing text and getting keywords or phrases\n",
    "- GPT models are also good at extracting entities that are context-specific.\n",
    "    - So if you are looking for a particular person's organization, not just an organization.\n",
    "\n",
    "![](figures/45514NER2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis\n",
    "\n",
    "- Understand how \"positive\" or \"negative\" a piece of text is.\n",
    "\n",
    "![](figures/main.png){width=400}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Segmentation\n",
    "\n",
    "- Often you might want to separate a long string of text into multiple sets of thoughts or topics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different GPT models\n",
    "\n",
    "- There are different models you can use from GPT\n",
    "- We won't consider the audio or visual models here\n",
    "- We can basically separate into two categories:\n",
    "    - GPT-3/4 super smart models\n",
    "    - davinci, babbage models that are the \"base\" models for GPT\n",
    "- Depending on your use case, you might want to to use one over the other\n",
    "    - Mostly because of cost as some of the base models can still do things effectively but for cheap\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To get started, get your API key from the website\n",
    "- Install `pip install openai`\n",
    "    - This gives you two things\n",
    "        - a Python package you can use\n",
    "        - a CLI program you can use in the terminal or in the notebook with magics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import json\n",
    "\n",
    "\n",
    "openai.api_key = \"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\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Don't ever put your api key in plain text\n",
    "# If you ever put this on the public web, others can use your key and spend your money!\n",
    "# Rather create an .env file and install python-dotenv\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "openai.api_key = os.getenv(\"API_KEY\")\n",
    "\n",
    "# And then never commit the .env file to github\n",
    "# Put it into your .gitignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Parameters\n",
    "\n",
    "- There are few things to know on the different options you have for using openai models\n",
    "- The first and most important thing is the model: https://platform.openai.com/docs/models/overview\n",
    "- Then there are some parameters that are important:\n",
    "\n",
    "`messages`\n",
    "A list of messages comprising the conversation so far. Can also include a python function\n",
    "\n",
    "```python\n",
    "messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Knock knock.\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Who's there?\"},\n",
    "    {\"role\": \"user\", \"content\": \"Orange.\"},\n",
    "]\n",
    "```\n",
    "\n",
    "- Messages usually come with `content` and a `role`\n",
    "- Roles:\n",
    "    - system\n",
    "    - user\n",
    "    - assistant\n",
    "    - function (might discuss this)\n",
    "\n",
    "\n",
    "`frequency_penalty`/`presence penalty`\n",
    "Defaults to 0\n",
    "Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.\n",
    "\n",
    "Frequency_penalty: This parameter is used to discourage the model from repeating the same words or phrases too frequently within the generated text. It is a value that is added to the log-probability of a token each time it occurs in the generated text. A higher frequency_penalty value will result in the model being more conservative in its use of repeated tokens.\n",
    "\n",
    "Presence_penalty: This parameter is used to encourage the model to include a diverse range of tokens in the generated text. It is a value that is subtracted from the log-probability of a token each time it is generated. A higher presence_penalty value will result in the model being more likely to generate tokens that have not yet been included in the generated text.\n",
    "\n",
    "Example: \n",
    "\n",
    "You ask: \"What is the temperature today?\"\n",
    "\n",
    "Low presence penalty: \"The temperature today is 75 degrees.\"\n",
    "\n",
    "High presence penalty: \"On this morrow, the amount of warmth is at 75 degrees.\"\n",
    "\n",
    "Low frequency penalty: \"The temperature today is a temperature that is the temperature today, 75 degrees.\"\n",
    "\n",
    "High frequency penalty: \"It is 75 degrees.\"\n",
    "\n",
    "\n",
    "`logit_bias`\n",
    "\n",
    "Accepts a json object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token.\n",
    "\n",
    "`max_tokens`\n",
    "\n",
    "The maximum number of tokens to generate in the chat completion.\n",
    "\n",
    "The total length of input tokens and generated tokens is limited by the model's context length. \n",
    "\n",
    "`n`\n",
    "\n",
    "How many chat completion choices to generate for each input message.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "`stop`\n",
    "string / array / null\n",
    "Optional\n",
    "Defaults to null\n",
    "Up to 4 sequences where the API will stop generating further tokens.\n",
    "\n",
    "`stream`\n",
    "boolean or null\n",
    "Optional\n",
    "Defaults to false\n",
    "If set, partial message deltas will be sent, like in ChatGPT. Tokens will be sent as data-only server-sent events as they become available, with the stream terminated by a data: [DONE] message. Example Python code.\n",
    "\n",
    "**`temperature`**\n",
    "number or null\n",
    "Optional\n",
    "Defaults to 1\n",
    "What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.\n",
    "\n",
    "We generally recommend altering this or top_p but not both.\n",
    "\n",
    "**`top_p`**\n",
    "number or null\n",
    "Optional\n",
    "Defaults to 1\n",
    "An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\n",
    "\n",
    "We generally recommend altering this or temperature but not both.\n",
    "\n",
    "`user`\n",
    "string\n",
    "Optional\n",
    "A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. Learn more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's have some fun now:\n",
    "\n",
    "prompt = \"What were the dates of full moons in 2019?\"\n",
    "\n",
    "chat_completion = openai.ChatCompletion.create(\n",
    "  model='gpt-3.5-turbo',\n",
    "  messages = [\n",
    "    {'role' : 'system', \"content\" : prompt},\n",
    "    ],\n",
    "  temperature=0.5,\n",
    "  max_tokens=500,\n",
    "  frequency_penalty=0,\n",
    "  presence_penalty=0,\n",
    "  stop=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject chat.completion id=chatcmpl-8BQ7AFW63uzfsZm9vKERlA1MonuSn at 0x7f9cc068da40> JSON: {\n",
       "  \"id\": \"chatcmpl-8BQ7AFW63uzfsZm9vKERlA1MonuSn\",\n",
       "  \"object\": \"chat.completion\",\n",
       "  \"created\": 1697732684,\n",
       "  \"model\": \"gpt-3.5-turbo-0613\",\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"index\": 0,\n",
       "      \"message\": {\n",
       "        \"role\": \"assistant\",\n",
       "        \"content\": \"The dates of full moons in 2019 are as follows:\\n\\n- January 21\\n- February 19\\n- March 21\\n- April 19\\n- May 18\\n- June 17\\n- July 16\\n- August 15\\n- September 14\\n- October 13\\n- November 12\\n- December 12\"\n",
       "      },\n",
       "      \"finish_reason\": \"stop\"\n",
       "    }\n",
       "  ],\n",
       "  \"usage\": {\n",
       "    \"prompt_tokens\": 19,\n",
       "    \"completion_tokens\": 72,\n",
       "    \"total_tokens\": 91\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dates of full moons in 2019 are as follows:\n",
      "\n",
      "- January 21\n",
      "- February 19\n",
      "- March 21\n",
      "- April 19\n",
      "- May 18\n",
      "- June 17\n",
      "- July 16\n",
      "- August 15\n",
      "- September 14\n",
      "- October 13\n",
      "- November 12\n",
      "- December 12\n"
     ]
    }
   ],
   "source": [
    "print(chat_completion['choices'][0]['message']['content'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In 2019, the dates of the full moons were as follows:\n",
      "\n",
      "- January 21\n",
      "- February 19\n",
      "- March 21\n",
      "- April 19\n",
      "- May 18\n",
      "- June 17\n",
      "- July 16\n",
      "- August 15\n",
      "- September 14\n",
      "- October 13\n",
      "- November 12\n",
      "- December 12\n"
     ]
    }
   ],
   "source": [
    "# let's do the same but with low and high temperatures:\n",
    "\n",
    "# Let's have some fun now:\n",
    "\n",
    "message = \"What were the dates of full moons in 2019?\"\n",
    "\n",
    "chat_completion = openai.ChatCompletion.create(\n",
    "  model='gpt-3.5-turbo',\n",
    "  messages = [\n",
    "    {'role' : 'system', \"content\" : message},\n",
    "    ],\n",
    "  temperature=1,\n",
    "  max_tokens=500,\n",
    "  frequency_penalty=0,\n",
    "  presence_penalty=0,\n",
    "  stop=None)\n",
    "\n",
    "print(chat_completion['choices'][0]['message']['content'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Werewolves are mythological creatures that are half-human and half-wolf. In folklore and popular culture, they are often depicted as humans who possess the ability to transform into a wolf or a wolf-like creature, usually during a full moon. This transformation is often involuntary and is accompanied by a violent and uncontrollable behavior. Legend suggests that werewolves are cursed or afflicted individuals who transform into wolves to hunt and kill prey. They are often depicted as creatures with enhanced physical strength, heightened senses, and a vulnerability to silver. Werewolf mythologies can vary across different cultures, with werewolf-like creatures appearing in legends and folklore around the world.\n"
     ]
    }
   ],
   "source": [
    "# let's do the same but with low and high temperatures:\n",
    "\n",
    "# Let's have some fun now:\n",
    "\n",
    "message = \"What are werewolves?\"\n",
    "\n",
    "chat_completion = openai.ChatCompletion.create(\n",
    "  model='gpt-3.5-turbo',\n",
    "  messages = [\n",
    "    {'role' : 'system', \"content\" : message},\n",
    "    ],\n",
    "  # temperature=2,\n",
    "  # top_p=0.01,\n",
    "  max_tokens=500,\n",
    "  frequency_penalty=0,\n",
    "  presence_penalty=-2,\n",
    "  stop=None)\n",
    "\n",
    "print(chat_completion['choices'][0]['message']['content'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject at 0x7f9ca06f9d60> JSON: {\n",
       "  \"prompt_tokens\": 13,\n",
       "  \"completion_tokens\": 130,\n",
       "  \"total_tokens\": 143\n",
       "}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_completion['usage']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing the Right Model\n",
    "\n",
    "- In many cases, cost is the biggest issue with using these models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tokens = chat_completion['usage']['prompt_tokens']\n",
    "output_tokens = chat_completion['usage']['completion_tokens']\n",
    "total_tokens = chat_completion['usage']['total_tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Input Cost (per 1000)</th>\n",
       "      <th>Output Cost (per 1000)</th>\n",
       "      <th>Cost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GPT-4</td>\n",
       "      <td>0.0300</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.008190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GPT-3.5</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.000280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>davinci</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>0.000286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>babbage</td>\n",
       "      <td>0.0004</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>0.000057</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Model  Input Cost (per 1000) Output Cost (per 1000)      Cost\n",
       "0    GPT-4                 0.0300                   0.06  0.008190\n",
       "1  GPT-3.5                 0.0015                  0.002  0.000280\n",
       "2  davinci                 0.0020                   <NA>  0.000286\n",
       "3  babbage                 0.0004                   <NA>  0.000057"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(\n",
    "    {'Model' : ['GPT-4','GPT-3.5', 'davinci', 'babbage'],\n",
    "     'Input Cost (per 1000)' : [0.03, 0.0015, 0.002, 0.0004],\n",
    "     'Output Cost (per 1000)' : [0.06, 0.002, pd.NA, pd.NA]}\n",
    ").assign(Cost = lambda df: (input_tokens/1000)*df['Input Cost (per 1000)'] + (output_tokens/1000)*df['Output Cost (per 1000)'].fillna(df['Input Cost (per 1000)']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- GPT-4 can get expensive QUICK, but it is also much better at what it does\n",
    "- babbage is cheap, but not as capable\n",
    "- babbage and davinci are usually used to train new models for specialized tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Writing prompts are just as important as understanding the code here\n",
    "- In essence, the precision of your prompt is making up for the fact that you don't need to write code, train models or anything like that\n",
    "- In return, you need to write a prompt as precisely and clearly as possible so that the model gives you the input you are looking for.\n",
    "- There are also a few quirks of how your write a prompt that are GPT-specific"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Tell ChatGPT what you want at the beginning AND end\n",
    "    - Before going into an explanation, the prompt should specify the task at the very beginning. \n",
    "    - Then it should repeat the task at the end again\n",
    "\n",
    "```\n",
    "Your task is to verify if the statement \"Several sources mention a chance of another large eruption\" is supported by a specific quote from the following set of snippets.\n",
    "---\n",
    "SNIPPETS\n",
    "[1] 14 percent chance of megaquake hitting Seattle, experts say\n",
    "SEATTLE - There's a 14 percent chance of a magnitude 9 Cascadia earthquake hitting Seattle in the next 50 years, the U.S. Geological Survey estimates. \"Unfortunately, we are unable to...\n",
    "\n",
    "[2] Earthquake experts lay out latest outlook for Seattle's 'Really Big One’\n",
    "“We say that there's approximately a 14% chance of another approximately magnitude-9 earthquake occurring in the next 50 years,” said a geophysicist at the University of Washington...\n",
    "---\n",
    "Is the statement \"Several sources mention a chance of another large eruption\" directly implied or stated by the snippets?\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Prime the output\n",
    "    - This is critical. You often want the output of the model to NOT be a conversation, but something in a data format. Priming the prompt which includes information in the output you want it in, makes it more likely that the model will output in that form.\n",
    "\n",
    "```\n",
    "John Smith is married to Lucy Smith. They have five kids, and he works as a software engineer at Microsoft. What search queries should I do to fact-check this? ## One possible search query is:\n",
    "\n",
    "1. How many kids do they have?\n",
    "2. What is Lucy's maiden name?\n",
    "\n",
    "```\n",
    "\n",
    "or\n",
    "\n",
    "```\n",
    "Here’s a bulleted list of key points:\\n-\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Using clear syntax and delineating different parts of the prompt\n",
    "    - Delineating different sources of information or steps\n",
    "\n",
    "```\n",
    "You will read a paragraph, and then issue queries to a search engine in order to fact-check it. Also explain the queries.\n",
    "---\n",
    "PARAGRAPH\n",
    "John Smith is married to Lucy Smith. They have five kids, and he works as a software engineer at Microsoft. What search queries should I do to fact-check this?\n",
    "---\n",
    "QUERIES\n",
    "```\n",
    "\n",
    "- Here you use `---` to separate out the paragraph of information you want to query\n",
    "\n",
    "4. Break the job down into smaller tasks\n",
    "5. Chain of Though Prompting\n",
    "    - Ask the model to break down its chain of thought and steps it took when answering the question:\n",
    "\n",
    "```\n",
    "Q:\n",
    "Who was the most decorated (maximum medals) individual athlete in the Olympic games that were held at Sydney? Take a step-by-step approach in your response, cite sources and give reasoning before sharing final answer in the below format: ANSWER is: <name>\n",
    "```\n",
    "\n",
    "```\n",
    "A:\n",
    "Step 1: Determine the year of the Sydney Olympic games. The Sydney Olympic games were held in the year 2000.\n",
    "\n",
    "Step 2: Find a list of individual athletes who won multiple medals in the 2000 Sydney Olympics.\n",
    "\n",
    "According to the official website of the Olympic Games, there were several individual athletes who won multiple medals in the 2000 Sydney Olympics. Some of the notable names include:\n",
    "• Ian Thorpe (Australia) - 3 gold, 2 silver, 0 bronze\n",
    "• Pieter van den Hoogenband (Netherlands) - 2 gold, 0 silver, 2 bronze\n",
    "• Lenny Krayzelburg (USA) - 3 gold, 0 silver, 0 bronze\n",
    "• Inge de Bruijn (Netherlands) - 3 gold, 1 silver, 0 bronze\n",
    "\n",
    "Step 3: Determine the athlete with the most medals. After reviewing the list of individual athletes who won multiple medals in the 2000 Sydney Olympics, we can see that the athlete with the most medals is Ian Thorpe from Australia, who won a total of 5 medals (3 gold and 2 silver).\n",
    "\n",
    "Therefore, the ANSWER is: Ian Thorpe.\n",
    "```\n",
    "\n",
    "6. Model Output\n",
    "    - Sometimes models \"hallucinate\"\n",
    "    - They can make things up or take things as true which aren't\n",
    "    - Simply asking in the prompt, \"Only give true statements,\" might not be enough\n",
    "    - Rather ask it for citations (or even better, inline citations)\n",
    "7. Grounding with context\n",
    "    - If you need analysis and not something creative, it is strongly encouraged to give the model data that it will draw its responses from. \n",
    "8. Giving Examples\n",
    "    - Sometimes it helps to give the model an example of what output you want \n",
    "    - Perhaps it's even a validated version of what you asked for before and then fixed.\n",
    "\n",
    "```\n",
    "messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Tell me all the vowels in this sentence.\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"There are 4 vowels in this sentence.\"},\n",
    "    {\"role\": \"user\", \"content\": \"How many vowels are in this sentence now?\"},\n",
    "]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting it out as data\n",
    "\n",
    "- It is important to specify how you want the data to be output.\n",
    "- In this case, it is useful to specify two things:\n",
    "    1. Tell it not to be conversational\n",
    "    2. Only answer in the form of a particular data form.\n",
    "        - In some JSON standard\n",
    "        - In CSV\n",
    "        - something else\n",
    "- Then you can take this text data and turn it into a dataset\n",
    "- This, however, can be a challenge\n",
    "    - The data GPT spits out may not easily be converted\n",
    "    - JSON needs to be validated\n",
    "    - CSVs might not work if the model decides to throw int some commas\n",
    "- Important to give it an example in the correct form\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training your own model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- GPT is trained on lots of data and is good at many tasks\n",
    "- But perhaps it's not great at those tasks in another language\n",
    "- Or you want to save money because you want the LLM to do a very particular thing\n",
    "- Sometimes you want to do a task that is specialized, and would benefit from extra training\n",
    "- In this case, you can train your own models\n",
    "- But beware, you need TRAINING DATA (will discuss training/testing in ML lecture)\n",
    "    - And you need a lot of it\n",
    "    - Since the base models are already partially trained, you need to provide it with enough instances such that it will actually \"respond\" to your new data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting data into the correct form\n",
    "\n",
    "- You need to get whatever data you have into `jsonl` format, which needs to look like this:\n",
    "\n",
    "```\n",
    "{\"prompt\": \"<prompt text>\", \"completion\": \"<ideal generated text>\"}\n",
    "{\"prompt\": \"<prompt text>\", \"completion\": \"<ideal generated text>\"}\n",
    "{\"prompt\": \"<prompt text>\", \"completion\": \"<ideal generated text>\"}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Extracting an entity from a set of sentences\n",
    "\n",
    "Let's say we want to extract the company name from a set of phrases about companies:\n",
    "\n",
    "1. I ordered a new laptop from Amazon, and it arrived in just two days.\n",
    "1. Apple's latest iPhone features an impressive camera with advanced computational photography.\n",
    "1. Coca-Cola is a global leader in the beverage industry, known for its iconic red cans.\n",
    "1. Microsoft's Windows 10 operating system received a major update last week.\n",
    "1. Tesla's electric cars are revolutionizing the automotive industry with their cutting-edge technology.\n",
    "1. Starbucks is my favorite coffee shop, and I can't resist their caramel macchiatos.\n",
    "1. Google's search engine is my go-to tool for finding information online.\n",
    "1. IBM is known for its innovation in the field of artificial intelligence and quantum computing.\n",
    "1. McDonald's has a diverse menu that caters to people with various food preferences.\n",
    "1. I recently purchased a pair of Nike sneakers for my daily workouts at the gym.\n",
    "\n",
    "With company names:\n",
    "\n",
    "1. Amazon\n",
    "2. Apple\n",
    "3. Coca-Cola\n",
    "4. Microsoft\n",
    "5. Tesla\n",
    "6. Starbucks\n",
    "7. Google\n",
    "8. IBM\n",
    "9. McDonald's\n",
    "10. Nike\n",
    "\n",
    "Let's create a a prompt that we can use to create the prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "\"I ordered a new laptop from Amazon, and it arrived in just two days.\",\n",
    "\"Apple's latest iPhone features an impressive camera with advanced computational photography.\",\n",
    "\"Coca-Cola is a global leader in the beverage industry, known for its iconic red cans.\",\n",
    "\"Microsoft's Windows 10 operating system received a major update last week.\",\n",
    "\"Tesla's electric cars are revolutionizing the automotive industry with their cutting-edge technology.\",\n",
    "\"Starbucks is my favorite coffee shop, and I can't resist their caramel macchiatos.\",\n",
    "\"Google's search engine is my go-to tool for finding information online.\",\n",
    "\"IBM is known for its innovation in the field of artificial intelligence and quantum computing.\",\n",
    "\"McDonald's has a diverse menu that caters to people with various food preferences.\",\n",
    "\"I recently purchased a pair of Nike sneakers for my daily workouts at the gym. \",\n",
    "\"Facebook is a popular social media platform where I stay connected with friends and family.\",\n",
    "\"Amazon Prime offers fast shipping and a vast selection of movies and TV shows.\",\n",
    "\"Uber has made transportation in the city so convenient with its ride-sharing services.\",\n",
    "\"Walmart is a one-stop shop for all my household needs, from groceries to electronics.\",\n",
    "\"I'm a loyal customer of Delta Airlines, and their service has always been exceptional.\",\n",
    "\"Adobe Creative Cloud provides a wide range of creative software for designers and artists.\",\n",
    "\"Toyota is known for producing reliable and fuel-efficient vehicles.\",\n",
    "\"I love browsing through the latest fashion trends on Zara's website.\",\n",
    "\"PayPal makes online transactions a breeze, ensuring secure payments.\",\n",
    "\"Verizon offers high-speed internet that keeps my family connected and entertained.\"\n",
    "]\n",
    "\n",
    "entities = [\n",
    "\"Amazon\",\n",
    "\"Apple\",\n",
    "\"Coca-Cola\",\n",
    "\"Microsoft\",\n",
    "\"Tesla\",\n",
    "\"Starbucks\",\n",
    "\"Google\",\n",
    "\"IBM\",\n",
    "\"McDonald's\",\n",
    "\"Nike\",\n",
    "\"Facebook\",\n",
    "\"Amazon\",\n",
    "\"Uber\",\n",
    "\"Walmart\",\n",
    "\"Delta\",\n",
    "\"Adobe\",\n",
    "\"Toyota\",\n",
    "\"Zara\",\n",
    "\"Paypal\",\n",
    "\"Verizon\"\n",
    "]\n",
    "\n",
    "prompt = lambda s: f\"You are a helpful assistant. Only answer in the form of RFC8259 compliant JSON. For this sentence, extract the entity of the company name. Now extract the entity of the company name in this sentence. Here is the sentence: {s}\"\n",
    "\n",
    "json_prompt = lambda s,e: f'{{\"messages\": [{{\"role\" : \"user\", \"content\" : \"{prompt(s)}\"}}, {{\"role\" : \"assistant\", \"content\" : \"{e}\"}}]}}'\n",
    "\n",
    "training_list = []\n",
    "\n",
    "for s,e in zip(sentences, entities):\n",
    "    training_list.append(json_prompt(s,e))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"messages\": [{\"role\" : \"user\", \"content\" : \"You are a helpful assistant. Only answer in the form of RFC8259 compliant JSON. For this sentence, extract the entity of the company name. Now extract the entity of the company name in this sentence. Here is the sentence: I ordered a new laptop from Amazon, and it arrived in just two days.\"}, {\"role\" : \"assistant\", \"content\" : \"Amazon\"}]}'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_gpt(dataset):\n",
    "    # Format error checks\n",
    "    format_errors = defaultdict(int)\n",
    "\n",
    "    for ex in dataset:\n",
    "        if not isinstance(ex, dict):\n",
    "            format_errors[\"data_type\"] += 1\n",
    "            continue\n",
    "            \n",
    "        messages = ex.get(\"messages\", None)\n",
    "        if not messages:\n",
    "            format_errors[\"missing_messages_list\"] += 1\n",
    "            continue\n",
    "            \n",
    "        for message in messages:\n",
    "            if \"role\" not in message or \"content\" not in message:\n",
    "                format_errors[\"message_missing_key\"] += 1\n",
    "            \n",
    "            if any(k not in (\"role\", \"content\", \"name\", \"function_call\") for k in message):\n",
    "                format_errors[\"message_unrecognized_key\"] += 1\n",
    "            \n",
    "            if message.get(\"role\", None) not in (\"system\", \"user\", \"assistant\", \"function\"):\n",
    "                format_errors[\"unrecognized_role\"] += 1\n",
    "                \n",
    "            content = message.get(\"content\", None)\n",
    "            function_call = message.get(\"function_call\", None)\n",
    "            \n",
    "            if (not content and not function_call) or not isinstance(content, str):\n",
    "                format_errors[\"missing_content\"] += 1\n",
    "        \n",
    "        if not any(message.get(\"role\", None) == \"assistant\" for message in messages):\n",
    "            format_errors[\"example_missing_assistant_message\"] += 1\n",
    "\n",
    "    if format_errors:\n",
    "        print(\"Found errors:\")\n",
    "        for k, v in format_errors.items():\n",
    "            print(f\"{k}: {v}\")\n",
    "    else:\n",
    "        print(\"No errors found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No errors found\n"
     ]
    }
   ],
   "source": [
    "validate_gpt([json.loads(i) for i in training_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['{\"messages\": [{\"role\" : \"user\", \"content\" : \"You are a helpful assistant. Only answer in the form of RFC8259 compliant JSON. For this sentence, extract the entity of the company name. Now extract the entity of the company name in this sentence. Here is the sentence: I ordered a new laptop from Amazon, and it arrived in just two days.\"}, {\"role\" : \"assistant\", \"content\" : \"Amazon\"}]}',\n",
       " '{\"messages\": [{\"role\" : \"user\", \"content\" : \"You are a helpful assistant. Only answer in the form of RFC8259 compliant JSON. For this sentence, extract the entity of the company name. Now extract the entity of the company name in this sentence. Here is the sentence: Apple\\'s latest iPhone features an impressive camera with advanced computational photography.\"}, {\"role\" : \"assistant\", \"content\" : \"Apple\"}]}',\n",
       " '{\"messages\": [{\"role\" : \"user\", \"content\" : \"You are a helpful assistant. Only answer in the form of RFC8259 compliant JSON. For this sentence, extract the entity of the company name. Now extract the entity of the company name in this sentence. Here is the sentence: Coca-Cola is a global leader in the beverage industry, known for its iconic red cans.\"}, {\"role\" : \"assistant\", \"content\" : \"Coca-Cola\"}]}',\n",
       " '{\"messages\": [{\"role\" : \"user\", \"content\" : \"You are a helpful assistant. Only answer in the form of RFC8259 compliant JSON. For this sentence, extract the entity of the company name. Now extract the entity of the company name in this sentence. Here is the sentence: Microsoft\\'s Windows 10 operating system received a major update last week.\"}, {\"role\" : \"assistant\", \"content\" : \"Microsoft\"}]}',\n",
       " '{\"messages\": [{\"role\" : \"user\", \"content\" : \"You are a helpful assistant. Only answer in the form of RFC8259 compliant JSON. For this sentence, extract the entity of the company name. Now extract the entity of the company name in this sentence. Here is the sentence: Tesla\\'s electric cars are revolutionizing the automotive industry with their cutting-edge technology.\"}, {\"role\" : \"assistant\", \"content\" : \"Tesla\"}]}',\n",
       " '{\"messages\": [{\"role\" : \"user\", \"content\" : \"You are a helpful assistant. Only answer in the form of RFC8259 compliant JSON. For this sentence, extract the entity of the company name. Now extract the entity of the company name in this sentence. Here is the sentence: Starbucks is my favorite coffee shop, and I can\\'t resist their caramel macchiatos.\"}, {\"role\" : \"assistant\", \"content\" : \"Starbucks\"}]}',\n",
       " '{\"messages\": [{\"role\" : \"user\", \"content\" : \"You are a helpful assistant. Only answer in the form of RFC8259 compliant JSON. For this sentence, extract the entity of the company name. Now extract the entity of the company name in this sentence. Here is the sentence: Google\\'s search engine is my go-to tool for finding information online.\"}, {\"role\" : \"assistant\", \"content\" : \"Google\"}]}',\n",
       " '{\"messages\": [{\"role\" : \"user\", \"content\" : \"You are a helpful assistant. Only answer in the form of RFC8259 compliant JSON. For this sentence, extract the entity of the company name. Now extract the entity of the company name in this sentence. Here is the sentence: IBM is known for its innovation in the field of artificial intelligence and quantum computing.\"}, {\"role\" : \"assistant\", \"content\" : \"IBM\"}]}',\n",
       " '{\"messages\": [{\"role\" : \"user\", \"content\" : \"You are a helpful assistant. Only answer in the form of RFC8259 compliant JSON. For this sentence, extract the entity of the company name. Now extract the entity of the company name in this sentence. Here is the sentence: McDonald\\'s has a diverse menu that caters to people with various food preferences.\"}, {\"role\" : \"assistant\", \"content\" : \"McDonald\\'s\"}]}',\n",
       " '{\"messages\": [{\"role\" : \"user\", \"content\" : \"You are a helpful assistant. Only answer in the form of RFC8259 compliant JSON. For this sentence, extract the entity of the company name. Now extract the entity of the company name in this sentence. Here is the sentence: I recently purchased a pair of Nike sneakers for my daily workouts at the gym. \"}, {\"role\" : \"assistant\", \"content\" : \"Nike\"}]}',\n",
       " '{\"messages\": [{\"role\" : \"user\", \"content\" : \"You are a helpful assistant. Only answer in the form of RFC8259 compliant JSON. For this sentence, extract the entity of the company name. Now extract the entity of the company name in this sentence. Here is the sentence: Facebook is a popular social media platform where I stay connected with friends and family.\"}, {\"role\" : \"assistant\", \"content\" : \"Facebook\"}]}']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_list[0:11]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now separate into training and testing and save to a jsonl file\n",
    "\n",
    "with open(\"training.jsonl\", 'w') as f:\n",
    "    for i in training_list[0:11]:\n",
    "        f.write(i + \"\\n\")\n",
    "\n",
    "with open(\"testing.jsonl\", 'w') as f:\n",
    "    for i in training_list[11:]:\n",
    "        f.write(i + \"\\n\")\n",
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<File file id=file-LJsrwQObFFUnt5QuTea8k88d at 0x7f9cc02e30e0> JSON: {\n",
       "  \"object\": \"file\",\n",
       "  \"id\": \"file-LJsrwQObFFUnt5QuTea8k88d\",\n",
       "  \"purpose\": \"fine-tune\",\n",
       "  \"filename\": \"file\",\n",
       "  \"bytes\": 4425,\n",
       "  \"created_at\": 1697734273,\n",
       "  \"status\": \"uploaded\",\n",
       "  \"status_details\": null\n",
       "}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openai.File.create(\n",
    "  file=open(\"training.jsonl\", 'rb'),\n",
    "  purpose='fine-tune'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<FineTuningJob fine_tuning.job id=ftjob-DwpFFJPzT8QNExVzwozOeGSN at 0x7fd7f823bcc0> JSON: {\n",
       "  \"object\": \"fine_tuning.job\",\n",
       "  \"id\": \"ftjob-DwpFFJPzT8QNExVzwozOeGSN\",\n",
       "  \"model\": \"gpt-3.5-turbo-0613\",\n",
       "  \"created_at\": 1697667332,\n",
       "  \"finished_at\": null,\n",
       "  \"fine_tuned_model\": null,\n",
       "  \"organization_id\": \"org-SYo0hDr5zySyDsBnhEoEVSH2\",\n",
       "  \"result_files\": [],\n",
       "  \"status\": \"validating_files\",\n",
       "  \"validation_file\": null,\n",
       "  \"training_file\": \"file-o4OLhd4QerroX3ptfxikgx95\",\n",
       "  \"hyperparameters\": {\n",
       "    \"n_epochs\": \"auto\"\n",
       "  },\n",
       "  \"trained_tokens\": null,\n",
       "  \"error\": null\n",
       "}"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openai.FineTuningJob.create(training_file=\"file-o4OLhd4QerroX3ptfxikgx95\", model=\"babbage\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<FineTuningJob fine_tuning.job id=ftjob-DwpFFJPzT8QNExVzwozOeGSN at 0x7f9ca0723bd0> JSON: {\n",
       "  \"object\": \"fine_tuning.job\",\n",
       "  \"id\": \"ftjob-DwpFFJPzT8QNExVzwozOeGSN\",\n",
       "  \"model\": \"gpt-3.5-turbo-0613\",\n",
       "  \"created_at\": 1697667332,\n",
       "  \"finished_at\": 1697668584,\n",
       "  \"fine_tuned_model\": \"ft:gpt-3.5-turbo-0613:personal::8B9RKvBF\",\n",
       "  \"organization_id\": \"org-SYo0hDr5zySyDsBnhEoEVSH2\",\n",
       "  \"result_files\": [\n",
       "    \"file-uttxKXtAZm9ith274iQilFcT\"\n",
       "  ],\n",
       "  \"status\": \"succeeded\",\n",
       "  \"validation_file\": null,\n",
       "  \"training_file\": \"file-o4OLhd4QerroX3ptfxikgx95\",\n",
       "  \"hyperparameters\": {\n",
       "    \"n_epochs\": 9\n",
       "  },\n",
       "  \"trained_tokens\": 7560,\n",
       "  \"error\": null\n",
       "}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List 10 fine-tuning jobs\n",
    "openai.FineTuningJob.list(limit=10)\n",
    "\n",
    "# Retrieve the state of a fine-tune\n",
    "openai.FineTuningJob.retrieve(\"ftjob-DwpFFJPzT8QNExVzwozOeGSN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"role\": \"assistant\",\n",
      "  \"content\": \"Google\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "completion = openai.ChatCompletion.create(\n",
    "  model=\"ft:gpt-3.5-turbo-0613:personal::8B9RKvBF\", # Change for real model \n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": f\"{prompt('Google is my favorite company')}\"},\n",
    "  ]\n",
    ")\n",
    "print(completion.choices[0].message)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
